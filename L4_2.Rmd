---
title: "Dimension Reduction Methods"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo = T, message = FALSE, results='hide', warning=FALSE}
library(ISLR)
library(pls)
library(caret)
```

Predict a baseball playerâ€™s salary on the basis of various statistics associated with performance in the previous year. Use `?Hitters` for more details. 

Ideally, a model should be evaluated on datasets that were not used to build or fine-tune the model, so that they provide an unbiased sense of model effectiveness. When a large amount of data is at hand, a set of samples can be set aside to evaluate the final model. However, when the number of samples is not large, a test set may be avoided because every sample may be needed for model building. Moreover, the size of the test set may not have sufficient power or precision to make reasonable judgements.

Last time we use all the data to build the models. This time we split the data into a training set and a test set. 

```{r}
data(Hitters)
# delete rows containing the missing data
Hitters <- na.omit(Hitters)

set.seed(2019)
trRows <- createDataPartition(Hitters$Salary,
                              p = .75,
                              list = F)

# training data
# matrix of predictors (glmnet uses input matrix)
x <- model.matrix(Salary~.,Hitters)[trRows,-1]
# vector of response
y <- Hitters$Salary[trRows]

# test data
x2 <- model.matrix(Salary~.,Hitters)[-trRows,-1]
y2 <- Hitters$Salary[-trRows]
```

## Principal components regression (PCR)

We fit the PCR model using the function `pcr()`.

```{r}
set.seed(2)
pcr.mod <- pcr(Salary~., 
               data = Hitters[trRows,],
               scale = TRUE, 
               validation = "CV")

summary(pcr.mod)

validationplot(pcr.mod, val.type="MSEP", legendpos = "topright")

predy2.pcr <- predict(pcr.mod, newdata = Hitters[-trRows,], 
                      ncomp = 18)
# test MSE
mean((predy2.pcr-y2)^2)
```


## Partial least squares (PLS)

We fit the PLS model using the function `plsr()`.
```{r}
set.seed(2)
pls.mod <- plsr(Salary~., 
                data = Hitters[trRows,], 
                scale = TRUE,  
                validation = "CV")

summary(pls.mod)
validationplot(pls.mod, val.type="MSEP", legendpos = "topright")

predy2.pls <- predict(pcr.mod, newdata = Hitters[-trRows,], 
                      ncomp = 14)
# test MSE
mean((predy2.pls-y2)^2)
```

## PCR and PLS using `caret`

### PCR 
```{r}
ctrl1 <- trainControl(method = "repeatedcv", number = 10, repeats = 5)

# Two ways for standardizing predictors
# train(..., preProc = c("center", "scale"))
set.seed(2)
pcr.fit <- train(x, y,
                 method = "pcr",
                 tuneLength = 18,
                 trControl = ctrl1,
                 preProc = c("center", "scale"))
# need to preprocess your data when using predict()
trans <- preProcess(x, method = c("center", "scale"))
predy2.pcr2 <- predict(pcr.fit$finalModel, newdata = predict(trans, x2), 
                       ncomp = pcr.fit$bestTune[[1]])
mean((predy2.pcr2-y2)^2)

# pcr(..., scale = TRUE)
set.seed(2)
pcr.fit2 <- train(x, y,
                  method = "pcr",
                  tuneLength = 18,
                  trControl = ctrl1,
                  scale = TRUE)

predy2.pcr3 <- predict(pcr.fit2$finalModel, newdata = x2, 
                       ncomp = pcr.fit2$bestTune[[1]])
mean((predy2.pcr3-y2)^2)

ggplot(pcr.fit, highlight = TRUE) + theme_bw()
# ggplot(pcr.fit2, highlight = TRUE) # the same plot
```

### PLS
```{r}
set.seed(2)
pls.fit <- train(x, y,
                 method = "pls",
                 tuneLength = 18,
                 trControl = ctrl1,
                 scale = TRUE)
predy2.pls2 <- predict(pls.fit$finalModel, newdata = x2, 
                       ncomp = pls.fit$bestTune[[1]])
mean((predy2.pls2-y2)^2)

ggplot(pls.fit, highlight = TRUE)
```

Here are some old codes on ridge, lasso and ordinary least squares.
```{r}
set.seed(2)
ridge.fit <- train(x, y,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 0, 
                                          lambda = exp(seq(-1, 10, length=100))),
                   # preProc = c("center", "scale"),
                   trControl = ctrl1)
predy2.ridge <- predict(ridge.fit$finalModel, newx = x2, 
                        s = ridge.fit$bestTune$lambda, type = "response")
mean((predy2.ridge-y2)^2)

set.seed(2)
lasso.fit <- train(x, y,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(-1, 5, length=100))),
                   # preProc = c("center", "scale"),
                   trControl = ctrl1)
predy2.lasso <- predict(lasso.fit$finalModel, newx = x2, 
                        s = lasso.fit$bestTune$lambda, type = "response")
mean((predy2.lasso-y2)^2)

set.seed(2)
lm.fit <- train(x, y,
                method = "lm",
                trControl = ctrl1)
predy2.lm <- predict(lm.fit$finalModel, newdata = data.frame(x2))
mean((predy2.lm-y2)^2)

```

Comparing the models based on resampling results.
```{r}
resamp <- resamples(list(lasso = lasso.fit, 
                         ridge = ridge.fit, 
                         pcr = pcr.fit, 
                         pls = pls.fit,
                         lm = lm.fit))
summary(resamp)

bwplot(resamp, metric = "RMSE")
```